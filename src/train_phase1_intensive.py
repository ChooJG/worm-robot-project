"""
Phase 1 ì§‘ì¤‘ í•™ìŠµ - ê°€ì¥ ê°„ë‹¨í•œ ì¼€ì´ìŠ¤ë¶€í„° í™•ì‹¤íˆ í•´ê²°
ë¡œë´‡ 1ê°œ, ì¥ì• ë¬¼ ì—†ìŒ, ëŒ€í­ ê°„ì†Œí™”
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from rl.agent import DQNAgent
from rl.trainer import DQNTrainer
from rl.demonstrations_extended import get_extended_demonstrations
from system import WormRobotSystem

def create_system_fn(rl_agent=None):
    """1ê°œ ë¡œë´‡ ì‹œìŠ¤í…œ ìƒì„±"""
    return WormRobotSystem(
        rl_agent=rl_agent, 
        num_robots=1,
        obstacles=None
    )

def main():
    print("\n" + "=" * 70)
    print("ğŸ¯ Phase 1 ì§‘ì¤‘ í•™ìŠµ - ì´ˆê°„ì†Œí™” ë²„ì „")
    print("=" * 70)
    print("ì „ëµ: ë¡œë´‡ 1ê°œë§Œ, ëŒ€ëŸ‰ ì—í”¼ì†Œë“œ, ê°•ë ¥í•œ ë³´ìƒ")
    print("=" * 70)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê·¹ë‹¨ì  ë‹¨ìˆœí™”)
    STATE_DIM = 13
    ACTION_DIM = 4  # 3 â†’ 4 (STAY ì¶”ê°€)
    
    agent = DQNAgent(
        state_dim=STATE_DIM,
        action_dim=ACTION_DIM,
        learning_rate=0.001,      # ë” ë¹ ë¥¸ í•™ìŠµ
        gamma=0.95,               # ë‹¨ê¸° ë³´ìƒì— ì§‘ì¤‘
        epsilon_start=1.0,
        epsilon_end=0.05,         # ë” ë§ì€ í™œìš©
        epsilon_decay=0.9998,     # ì²œì²œíˆ ê°ì†Œ
        use_target_net=True,      # ì•ˆì •í™”ë¥¼ ìœ„í•´ í™œì„±í™”!
        target_update_freq=100,   # ìì£¼ ì—…ë°ì´íŠ¸
        device="cpu"
    )
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = DQNTrainer(
        agent=agent,
        create_system_fn=create_system_fn,
        num_episodes=30000,       # 10ë°° ì¦ê°€!
        termination_time=80,      # ì ë‹¹íˆ ì¤„ì„
        batch_size=128,           # ë” í° ë°°ì¹˜
        buffer_size=100000,       # ëŒ€ìš©ëŸ‰ ë²„í¼
        log_interval=100,
        save_interval=1000,
        model_path="outputs/phase1_intensive.pth",
        use_tensorboard=True
    )
    
    # Happy Path ëŒ€ëŸ‰ ì¶”ê°€
    print(f"\nğŸ“– Happy Path ëŒ€ëŸ‰ ì¶”ê°€ ì¤‘...")
    demos = get_extended_demonstrations(num_robots=1, num_random=500)  # 5ë°° ì¦ê°€!
    trainer.replay_buffer.add_demonstrations(demos)
    print(f"   Demo ë¹„ìœ¨: {trainer.replay_buffer.get_demo_ratio()*100:.1f}%")
    print(f"   ì´ {len(demos)}ê°œì˜ ì„±ê³µ ê²½í—˜!")
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ Phase 1 ì§‘ì¤‘ í•™ìŠµ ì‹œì‘!\n")
    try:
        stats = trainer.train()
        
        # í‰ê°€
        print(f"\nğŸ“Š í‰ê°€ ì¤‘...")
        eval_stats = trainer.evaluate(num_episodes=20)
        
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"   ìµœì¢… ìŠ¹ë¥ : {eval_stats['win_rate']*100:.1f}%")
        print(f"   í‰ê·  ë³´ìƒ: {eval_stats['avg_reward']:.1f}")
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ë¨!")
        trainer._save_model()
    
    print("\n" + "=" * 70)
    print("ëª¨ë¸ ì €ì¥: outputs/phase1_intensive.pth")
    print("=" * 70)

if __name__ == "__main__":
    main()

