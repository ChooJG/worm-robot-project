"""
ì´ˆê°„ë‹¨ Phase 1 í•™ìŠµ V2
- Prioritized Experience Replay í™œì„±í™”
- ì¥ì• ë¬¼ ì¶©ëŒ í˜ë„í‹° ê·¹ëŒ€í™”
- ë” ë‚˜ì€ íƒí—˜ ì „ëµ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from rl.agent import DQNAgent
from rl.replay_buffer import PrioritizedReplayBuffer  # ë³€ê²½!
from rl.demonstrations_extended import get_extended_demonstrations
from system import WormRobotSystem
from config import STATUS_WIN, STATUS_FAIL, STATUS_RUNNING
import numpy as np

def create_system_fn(rl_agent=None):
    """ë¡œë´‡ 1ê°œ, ì¥ì• ë¬¼ 1ê°œ ì‹œìŠ¤í…œ"""
    return WormRobotSystem(
        rl_agent=rl_agent, 
        num_robots=1,
        obstacles=[(0, 1)]  # ì¤‘ì•™ ê·¼ì²˜ì— ì¥ì• ë¬¼ 1ê°œ
    )

class ImprovedDQNTrainer:
    """
    ê°œì„ ëœ DQN Trainer
    - Prioritized Experience Replay
    - ì¥ì• ë¬¼ ì¶©ëŒ ì‹œ í° í˜ë„í‹°
    """
    
    def __init__(self, agent, create_system_fn, num_episodes=20000, 
                 termination_time=80, batch_size=128, buffer_size=100000,
                 log_interval=100, save_interval=1000, model_path="outputs/simple_phase1_v2.pth"):
        self.agent = agent
        self.create_system_fn = create_system_fn
        self.num_episodes = num_episodes
        self.termination_time = termination_time
        self.batch_size = batch_size
        self.log_interval = log_interval
        self.save_interval = save_interval
        self.model_path = model_path
        
        # Prioritized Replay Buffer!
        self.replay_buffer = PrioritizedReplayBuffer(capacity=buffer_size)
        
        # í†µê³„
        self.stats = {
            "episode_rewards": [],
            "episode_steps": [],
            "episode_losses": [],
            "success_count": 0,
            "fail_count": 0
        }
    
    def train(self):
        """í•™ìŠµ ë£¨í”„"""
        print("í•™ìŠµ ì‹œì‘...")
        
        for episode in range(self.num_episodes):
            episode_reward, episode_steps, status = self._run_episode()
            
            # í†µê³„ ê¸°ë¡
            self.stats["episode_rewards"].append(episode_reward)
            self.stats["episode_steps"].append(episode_steps)
            
            if status == STATUS_WIN:
                self.stats["success_count"] += 1
            elif status == STATUS_FAIL:
                self.stats["fail_count"] += 1
            
            # í•™ìŠµ (ë²„í¼ì— ì¶©ë¶„í•œ ê²½í—˜ì´ ìˆì„ ë•Œ)
            if len(self.replay_buffer.buffer) >= self.batch_size:
                # sample()ì€ íŠœí”Œ ë°˜í™˜: (states, actions, rewards, next_states, dones)
                # agent.train()ì€ ë¦¬ìŠ¤íŠ¸ ê¸°ëŒ€: [(s, a, r, ns, d), ...]
                states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
                batch = list(zip(states, actions, rewards, next_states, dones))
                loss = self.agent.train(batch)
                self.stats["episode_losses"].append(loss)
            
            # Epsilon ê°ì†Œ
            self.agent.update_epsilon()
            
            # ë¡œê·¸ ì¶œë ¥
            if (episode + 1) % self.log_interval == 0:
                recent = min(self.log_interval, len(self.stats["episode_rewards"]))
                avg_reward = sum(self.stats["episode_rewards"][-recent:]) / recent
                avg_steps = sum(self.stats["episode_steps"][-recent:]) / recent
                avg_loss = sum(self.stats["episode_losses"][-recent:]) / recent if self.stats["episode_losses"] else 0
                
                print(
                    f"Ep {episode + 1:5d}/{self.num_episodes} | "
                    f"Reward: {avg_reward:7.1f} | "
                    f"Steps: {avg_steps:4.1f} | "
                    f"Loss: {avg_loss:.4f} | "
                    f"Îµ: {self.agent.epsilon:.3f} | "
                    f"Win: {self.stats['success_count']:4d} | "
                    f"Fail: {self.stats['fail_count']:4d}"
                )
            
            # ëª¨ë¸ ì €ì¥
            if (episode + 1) % self.save_interval == 0:
                self.agent.save(self.model_path)
        
        # ìµœì¢… ì €ì¥
        self.agent.save(self.model_path)
        
        print("\n" + "=" * 60)
        print("í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì„±ê³µ: {self.stats['success_count']}")
        print(f"ì´ ì‹¤íŒ¨: {self.stats['fail_count']}")
        print(f"ì„±ê³µë¥ : {self.stats['success_count'] / self.num_episodes * 100:.1f}%")
        print("=" * 60)
        
        return self.stats
    
    def _run_episode(self):
        """ì—í”¼ì†Œë“œ ì‹¤í–‰ (Step-by-Step)"""
        system = self.create_system_fn(rl_agent=self.agent)
        num_robots = len(system.robots)
        
        episode_reward = 0.0
        step_count = 0
        max_steps = self.termination_time
        
        # ì—í”¼ì†Œë“œ ë£¨í”„
        while not system.is_done() and step_count < max_steps:
            # í˜„ì¬ ìƒíƒœ
            current_states = {}
            for rid in range(num_robots):
                if rid in system.environment.state.robot_positions:
                    state = system.get_state_for_robot(rid)
                    current_states[rid] = state
            
            # í–‰ë™ ì„ íƒ
            actions = {}
            for rid in current_states.keys():
                action = self.agent.get_action(current_states[rid])
                actions[rid] = action
            
            # ìŠ¤í… ì‹¤í–‰
            observations, rewards, done, status = system.step(actions)
            
            # ë‹¤ìŒ ìƒíƒœ
            next_states = {}
            for rid in range(num_robots):
                if rid in system.environment.state.robot_positions:
                    state = system.get_state_for_robot(rid)
                    next_states[rid] = state
            
            # ê²½í—˜ ì €ì¥ (ì¥ì• ë¬¼ ì¶©ëŒ ì‹œ í° í˜ë„í‹°!)
            step_reward = 0.0
            for rid in current_states.keys():
                if rid in next_states and rid in rewards:
                    robot_reward = rewards[rid]
                    
                    # ì‹¤íŒ¨ íŒì •
                    if done and status == STATUS_FAIL:
                        # ì¥ì• ë¬¼ ì¶©ëŒ í˜ë„í‹° ê·¹ëŒ€í™”!!!
                        robot_reward -= 500.0  # 50.0 â†’ 500.0
                    elif done and status == STATUS_WIN:
                        # ì„±ê³µ ë³´ë„ˆìŠ¤
                        robot_reward += 300.0
                    
                    step_reward += robot_reward
                    
                    # Prioritized Replay Bufferì— ì¶”ê°€
                    self.replay_buffer.add(
                        current_states[rid],
                        actions[rid],
                        robot_reward,
                        next_states[rid],
                        float(done)
                    )
            
            episode_reward += step_reward
            step_count += 1
            
            if done:
                break
        
        avg_reward = episode_reward / num_robots if num_robots > 0 else 0.0
        final_status = system.get_status()
        
        return avg_reward, step_count, final_status
    
    def evaluate(self, num_episodes=50):
        """í‰ê°€"""
        print("\n" + "=" * 60)
        print(f"í‰ê°€ ì‹œì‘ ({num_episodes} ì—í”¼ì†Œë“œ)")
        print("=" * 60)
        
        success_count = 0
        fail_count = 0
        total_rewards = []
        total_steps = []
        
        original_epsilon = self.agent.epsilon
        self.agent.epsilon = 0.0  # í‰ê°€ ì‹œì—ëŠ” íƒí—˜ ì—†ìŒ
        
        for _ in range(num_episodes):
            reward, steps, status = self._run_episode()
            total_rewards.append(reward)
            total_steps.append(steps)
            
            if status == STATUS_WIN:
                success_count += 1
            elif status == STATUS_FAIL:
                fail_count += 1
        
        self.agent.epsilon = original_epsilon
        
        avg_reward = sum(total_rewards) / num_episodes
        avg_steps = sum(total_steps) / num_episodes
        win_rate = success_count / num_episodes
        
        print(f"í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
        print(f"í‰ê·  ìŠ¤í…: {avg_steps:.1f}")
        print(f"ìŠ¹ë¥ : {win_rate * 100:.1f}%")
        print("=" * 60)
        
        return {
            "avg_reward": avg_reward,
            "avg_steps": avg_steps,
            "win_rate": win_rate
        }

def main():
    print("\n" + "=" * 70)
    print("ğŸ¯ ì´ˆê°„ë‹¨ Phase 1 í•™ìŠµ V2 (ê°œì„ íŒ)")
    print("=" * 70)
    print("ê°œì„  ì‚¬í•­:")
    print("  1. Prioritized Experience Replay í™œì„±í™”")
    print("  2. ì¥ì• ë¬¼ ì¶©ëŒ í˜ë„í‹° 10ë°° ì¦ê°€ (-50 â†’ -500)")
    print("  3. ì„±ê³µ ë³´ìƒ ê°•í™”")
    print("=" * 70)
    
    # DQN ì—ì´ì „íŠ¸
    agent = DQNAgent(
        state_dim=13,
        action_dim=4,  # 3 â†’ 4 (STAY ì¶”ê°€)
        learning_rate=0.0005,     # 0.001 â†’ 0.0005 (ë” ì•ˆì •ì )
        gamma=0.95,
        epsilon_start=1.0,
        epsilon_end=0.1,           # 0.05 â†’ 0.1 (ë” ë§ì€ íƒí—˜)
        epsilon_decay=0.9995,      # 0.9997 â†’ 0.9995 (ì¡°ê¸ˆ ë¹ ë¥¸ ê°ì†Œ)
        use_target_net=True,
        device="cpu"
    )
    
    # íŠ¸ë ˆì´ë„ˆ
    trainer = ImprovedDQNTrainer(
        agent=agent,
        create_system_fn=create_system_fn,
        num_episodes=25000,        # 20000 â†’ 25000
        termination_time=80,
        batch_size=128,
        buffer_size=100000,
        log_interval=100,
        save_interval=1000,
        model_path="outputs/simple_phase1_v2.pth"
    )
    
    # Happy Path ì¶”ê°€
    print(f"\nğŸ“– Happy Path ì¶”ê°€ ì¤‘...")
    demos = get_extended_demonstrations(num_robots=1, num_random=500)
    trainer.replay_buffer.add_demonstrations(demos)
    print(f"   âœ… ì´ {len(demos)}ê°œì˜ ì„±ê³µ ê²½í—˜ ì¶”ê°€!")
    
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘!\n")
    
    try:
        stats = trainer.train()
        
        print(f"\nğŸ“Š ìµœì¢… í‰ê°€")
        eval_stats = trainer.evaluate(num_episodes=50)
        
        print(f"\n" + "=" * 70)
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print("=" * 70)
        print(f"ìµœì¢… ìŠ¹ë¥ :   {eval_stats['win_rate']*100:.1f}%")
        print(f"í‰ê·  ë³´ìƒ:   {eval_stats['avg_reward']:.1f}")
        print(f"í‰ê·  ìŠ¤í…:   {eval_stats['avg_steps']:.1f}")
        print("=" * 70)
        
        if eval_stats['win_rate'] >= 0.3:
            print("\nğŸ‰ ì„±ê³µ! Phase 1ì„ ì¶©ë¶„íˆ í•™ìŠµí–ˆìŠµë‹ˆë‹¤!")
        elif eval_stats['win_rate'] >= 0.1:
            print("\nâš ï¸ ë¶€ë¶„ ì„±ê³µ. ë” í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            print("\nâŒ í•™ìŠµ ì‹¤íŒ¨. ì„¤ì •ì„ ì¬ê²€í† í•´ì•¼ í•©ë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ë¨!")
        agent.save(trainer.model_path)
    
    print(f"\nì €ì¥ëœ ëª¨ë¸: {trainer.model_path}\n")

if __name__ == "__main__":
    main()

