"""
Worm Robot Simulation - RL Trainer
DQN í•™ìŠµ ë£¨í”„ êµ¬í˜„ (ê°„ë‹¨í•œ ë²„ì „)
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pypdevs.simulator import Simulator
from rl.replay_buffer import ReplayBuffer
from config import STATUS_RUNNING, STATUS_WIN, STATUS_FAIL

try:
    from torch.utils.tensorboard import SummaryWriter  # type: ignore
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("âš ï¸  TensorBoardë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì„¤ì¹˜í•˜ì„¸ìš”: pip3 install tensorboard")


class DQNTrainer:
    """
    DQN í•™ìŠµ ë£¨í”„ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    
    ì£¼ì˜: í˜„ì¬ëŠ” ì—í”¼ì†Œë“œ ì „ì²´ë¥¼ ì‹¤í–‰ í›„ ë³´ìƒ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•œ ë²„ì „
    """

    def __init__(
        self,
        agent,
        create_system_fn,
        num_episodes=1000,
        termination_time=100,
        batch_size=32,
        buffer_size=10000,
        log_interval=10,
        save_interval=100,
        model_path="models/dqn_worm_robot.pth",
        use_tensorboard=True,
        tensorboard_dir="runs/worm_robot_dqn"
    ):
        """
        Args:
            agent: DQN ì—ì´ì „íŠ¸
            create_system_fn: WormRobotSystemì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
            num_episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
            termination_time: ì‹œë®¬ë ˆì´ì…˜ ìµœëŒ€ ì‹œê°„ (ì´ˆ)
            batch_size: ë°°ì¹˜ í¬ê¸°
            buffer_size: Replay Buffer í¬ê¸°
            log_interval: ë¡œê·¸ ì¶œë ¥ ê°„ê²©
            save_interval: ëª¨ë¸ ì €ì¥ ê°„ê²©
            model_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            use_tensorboard: TensorBoard ì‚¬ìš© ì—¬ë¶€
            tensorboard_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
        """
        self.agent = agent
        self.create_system_fn = create_system_fn
        self.num_episodes = num_episodes
        self.termination_time = termination_time
        self.batch_size = batch_size
        self.log_interval = log_interval
        self.save_interval = save_interval
        self.model_path = model_path
        
        # Replay Buffer
        self.replay_buffer = ReplayBuffer(capacity=buffer_size)
        
        # TensorBoard
        self.writer = None
        if use_tensorboard and TENSORBOARD_AVAILABLE:
            self.writer = SummaryWriter(tensorboard_dir)
            print(f"ğŸ“Š TensorBoard ë¡œê¹… í™œì„±í™”: {tensorboard_dir}")
            print(f"   ì‹¤í–‰: tensorboard --logdir=runs")
        
        # í†µê³„
        self.stats = {
            "episode_rewards": [],
            "episode_steps": [],
            "episode_losses": [],
            "success_count": 0,
            "fail_count": 0
        }

    def train(self):
        """í•™ìŠµ ë£¨í”„ ì‹¤í–‰"""
        print("=" * 60)
        print("DQN í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        print(f"ì—í”¼ì†Œë“œ ìˆ˜: {self.num_episodes}")
        print(f"ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {self.termination_time}ì´ˆ")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ì´ˆê¸° Epsilon: {self.agent.epsilon:.3f}")
        print("=" * 60)
        
        for episode in range(self.num_episodes):
            episode_reward, episode_steps, episode_status = self._run_episode()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["episode_rewards"].append(episode_reward)
            self.stats["episode_steps"].append(episode_steps)
            
            if episode_status == STATUS_WIN:
                self.stats["success_count"] += 1
            elif episode_status == STATUS_FAIL:
                self.stats["fail_count"] += 1
            
            # í•™ìŠµ (ë°°ì¹˜ê°€ ì¶©ë¶„íˆ ìŒ“ì´ë©´)
            if len(self.replay_buffer) >= self.batch_size:
                total_loss = 0.0
                # ì—¬ëŸ¬ ë²ˆ í•™ìŠµ
                for _ in range(5):
                    # ReplayBuffer.sample()ì€ (states, actions, rewards, next_states, dones) ë°˜í™˜
                    # agent.train()ì€ [(s,a,r,s',d), ...] í˜•íƒœ ê¸°ëŒ€
                    states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
                    batch = list(zip(states, actions, rewards, next_states, dones))
                    loss = self.agent.train(batch)
                    total_loss += loss
                self.stats["episode_losses"].append(total_loss / 5)
            else:
                self.stats["episode_losses"].append(0.0)
            
            # Epsilon ê°ì†Œ
            self.agent.update_epsilon()
            
            # TensorBoard ë¡œê¹…
            if self.writer is not None:
                self.writer.add_scalar('Reward/episode', episode_reward, episode)
                self.writer.add_scalar('Steps/episode', episode_steps, episode)
                self.writer.add_scalar('Loss/episode', self.stats["episode_losses"][-1], episode)
                self.writer.add_scalar('Epsilon', self.agent.epsilon, episode)
                self.writer.add_scalar('Success/total', self.stats["success_count"], episode)
                self.writer.add_scalar('Fail/total', self.stats["fail_count"], episode)
                
                # ì„±ê³µ/ì‹¤íŒ¨ë¥¼ 0 ë˜ëŠ” 1ë¡œ ê¸°ë¡
                if episode_status == STATUS_WIN:
                    self.writer.add_scalar('Result/win', 1, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                elif episode_status == STATUS_FAIL:
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/fail', 1, episode)
            
            # ë¡œê·¸ ì¶œë ¥
            if (episode + 1) % self.log_interval == 0:
                recent = self.log_interval
                avg_reward = sum(self.stats["episode_rewards"][-recent:]) / recent
                avg_steps = sum(self.stats["episode_steps"][-recent:]) / recent
                avg_loss = sum(self.stats["episode_losses"][-recent:]) / recent
                
                print(
                    f"Ep {episode + 1:4d}/{self.num_episodes} | "
                    f"Reward: {avg_reward:6.1f} | "
                    f"Steps: {avg_steps:4.1f} | "
                    f"Loss: {avg_loss:.4f} | "
                    f"Îµ: {self.agent.epsilon:.3f} | "
                    f"Win: {self.stats['success_count']:3d} | "
                    f"Fail: {self.stats['fail_count']:3d}"
                )
            
            # ëª¨ë¸ ì €ì¥
            if (episode + 1) % self.save_interval == 0:
                self._save_model()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self._save_model()
        
        # TensorBoard writer ì¢…ë£Œ
        if self.writer is not None:
            self.writer.close()
            print("\nğŸ“Š TensorBoard ë¡œê·¸ ì €ì¥ ì™„ë£Œ")
        
        print("\n" + "=" * 60)
        print("í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì„±ê³µ: {self.stats['success_count']}")
        print(f"ì´ ì‹¤íŒ¨: {self.stats['fail_count']}")
        print(f"ì„±ê³µë¥ : {self.stats['success_count'] / self.num_episodes * 100:.1f}%")
        print("=" * 60)
        
        return self.stats

    def _run_episode(self):
        """
        ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰
        
        ê°„ë‹¨í•œ ë²„ì „: ì—í”¼ì†Œë“œ ì „ì²´ ì‹¤í–‰ í›„ ìµœì¢… ìƒíƒœ ê¸°ë°˜ ë³´ìƒ
        í–¥í›„ ê°œì„ : ìŠ¤í…ë³„ ê²½í—˜ ìˆ˜ì§‘
        """
        # ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ìƒì„± (ëœë¤ ì´ˆê¸°í™”)
        system = self.create_system_fn(rl_agent=self.agent)
        
        # ì´ˆê¸° ìƒíƒœ ìˆ˜ì§‘
        initial_states = {}
        for rid in range(system.controller.num_robots):
            if rid in system.environment.state.robot_positions:
                obs = system.environment._generate_observations()[rid]
                state = system.controller._observation_to_state(obs)
                initial_states[rid] = state
        
        # ì‹œë®¬ë ˆì´í„° ì„¤ì • ë° ì‹¤í–‰
        sim = Simulator(system)
        sim.setClassicDEVS()
        sim.setTerminationTime(self.termination_time)
        sim.simulate()
        
        # ìµœì¢… ìƒíƒœ ë° ë³´ìƒ ìˆ˜ì§‘
        final_status = system.environment.state.status
        step_count = system.environment.state.step_count
        
        # ê° ë¡œë´‡ì˜ ìµœì¢… ìƒíƒœ
        final_states = {}
        for rid in range(system.controller.num_robots):
            if rid in system.environment.state.robot_positions:
                obs = system.environment._generate_observations()[rid]
                state = system.controller._observation_to_state(obs)
                final_states[rid] = state
        
        # ë³´ìƒ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)
        total_reward = 0.0
        for rid in initial_states.keys():
            if rid in system.environment.state.robot_positions:
                pos = system.environment.state.robot_positions[rid]
                tail = pos["tail"]
                head = pos["head"]
                goal_head = system.environment.robot_goals.get(rid, (0, 0))
                
                # ê±°ë¦¬ ê³„ì‚°
                tail_dist = abs(tail[0]) + abs(tail[1])
                head_dist = abs(head[0] - goal_head[0]) + abs(head[1] - goal_head[1])
                total_dist = tail_dist + head_dist
                
                # ê¸°ë³¸ ë³´ìƒ: ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ë³´ìƒ
                reward = (12 - total_dist) * 10  # 0~120ì 
                
                # ì„±ê³µ ë³´ë„ˆìŠ¤
                if final_status == STATUS_WIN:
                    reward += 300.0
                elif final_status == STATUS_FAIL:
                    reward -= 100.0
                
                # ë¶€ë¶„ ì„±ê³µ ë³´ë„ˆìŠ¤
                if tail == (0, 0):
                    reward += 50.0  # ë’·ë°œ ë„ì°©
                if head == goal_head:
                    reward += 50.0  # ì•ë°œ ë„ì°©
            else:
                reward = -100.0  # ë¡œë´‡ì´ ì‚¬ë¼ì§
            
            total_reward += reward
            
            # ê²½í—˜ ì €ì¥ (ì´ˆê¸° ìƒíƒœ â†’ ìµœì¢… ìƒíƒœ)
            if rid in initial_states and rid in final_states:
                # í–‰ë™ì€ ê°„ëµí™” (ì‹¤ì œë¡œëŠ” ê° ë¡œë´‡ì˜ í–‰ë™ ê¸°ë¡ í•„ìš”)
                action = 0  # ì„ì‹œ
                done = (final_status != STATUS_RUNNING)
                
                self.replay_buffer.add(
                    initial_states[rid],
                    action,
                    reward,
                    final_states[rid],
                    float(done)
                )
        
        return total_reward, step_count, final_status

    def _save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(os.path.dirname(self.model_path) if os.path.dirname(self.model_path) else "models", exist_ok=True)
        self.agent.save(self.model_path)

    def evaluate(self, num_episodes=10):
        """í•™ìŠµëœ ì—ì´ì „íŠ¸ í‰ê°€"""
        print("\n" + "=" * 60)
        print(f"í‰ê°€ ì‹œì‘ ({num_episodes} ì—í”¼ì†Œë“œ)")
        print("=" * 60)
        
        success_count = 0
        total_rewards = []
        total_steps = []
        
        # ì›ë˜ epsilon ì €ì¥
        original_epsilon = self.agent.epsilon
        self.agent.epsilon = 0.0  # í‰ê°€ ì‹œì—ëŠ” íƒí—˜ ì•ˆ í•¨
        
        for episode in range(num_episodes):
            reward, steps, status = self._run_episode()
            total_rewards.append(reward)
            total_steps.append(steps)
            
            if status == STATUS_WIN:
                success_count += 1
        
        # Epsilon ë³µì›
        self.agent.epsilon = original_epsilon
        
        avg_reward = sum(total_rewards) / num_episodes
        avg_steps = sum(total_steps) / num_episodes
        success_rate = success_count / num_episodes * 100
        
        print(f"í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
        print(f"í‰ê·  ìŠ¤í…: {avg_steps:.1f}")
        print(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        print("=" * 60)
        
        return {
            "success_rate": success_rate,
            "avg_reward": avg_reward,
            "avg_steps": avg_steps
        }
